{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fe007551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textwrap import wrap\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "from spacy.cli import download\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81dac79",
   "metadata": {},
   "source": [
    "## Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "522105bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Initial ONET Data\n",
    "\n",
    "def create_onet_soc_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This takes the onet task statements and merges them with the SOC structure in order to get the SOC major group titles mapped to the tasks.\n",
    "        It then renames columns for better data usage.\n",
    "        It then normalizes the task names by making them all lowercase and stripping whitespace. \n",
    "        It then creates a new column to count the number of occurrences of each task in an occupation, and in an SOC title.\n",
    "        This originally did not rename columns or create a column for n_occurances_soc.\n",
    "    \n",
    "    Args:\n",
    "        onet_path (str): Path to the O*NET task statements CSV file\n",
    "        soc_path (str): Path to the SOC structure CSV file\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Merged DataFrame containing O*NET data with SOC major group titles\n",
    "    \"\"\"\n",
    "\n",
    "    # Read and process O*NET data\n",
    "    onet_df = pd.read_csv(\"../original_data/onet_task_statements.csv\")\n",
    "    onet_df[\"soc_group_code\"] = onet_df[\"O*NET-SOC Code\"].str[:2]\n",
    "    \n",
    "    # Read and process SOC data\n",
    "    soc_df = pd.read_csv(\"../original_data/SOC_Structure.csv\")\n",
    "    soc_df = soc_df.dropna(subset=['Major Group'])\n",
    "    soc_df[\"soc_group_code\"] = soc_df[\"Major Group\"].str[:2]\n",
    "    \n",
    "    # Merge datasets\n",
    "    task_soc_df = onet_df.merge(\n",
    "        soc_df[['soc_group_code', 'SOC or O*NET-SOC 2019 Title']],\n",
    "        on='soc_group_code',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Rename columns for better usability\n",
    "    task_soc_df.rename(columns={\n",
    "    \"O*NET-SOC Code\": \"occ_group_code\",\n",
    "    \"Title\": \"title\",\n",
    "    \"Task ID\": \"task_id\",\n",
    "    \"Task\": \"task\",\n",
    "    \"Task Type\": \"task_type\",\n",
    "    \"Incumbents Responding\": \"n_responding\",\n",
    "    \"Date\": \"date\",\n",
    "    \"Domain Source\": \"domain_source\",\n",
    "    \"SOC or O*NET-SOC 2019 Title\": \"soc_title\",\n",
    "    }, inplace=True)\n",
    "\n",
    "    task_soc_df[\"task_normalized\"] = task_soc_df[\"task\"].str.lower().str.strip()\n",
    "    task_soc_df[\"n_occurrences\"] = task_soc_df.groupby(\"task_normalized\")[\"title\"].transform(\"nunique\")\n",
    "    task_soc_df[\"n_occurrences_soc\"] = task_soc_df.groupby(\"task_normalized\")[\"soc_title\"].transform(\"nunique\")\n",
    "\n",
    "    return task_soc_df\n",
    "\n",
    "task_soc_df = create_onet_soc_data()\n",
    "#display(task_soc_df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09c1e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Claude data\n",
    "def add_claude_pct(df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This loads in the tasks and percentage of occurrences from the Claude data, and merges it with the tasks in our data set we already have. \n",
    "        It then normalizes the percentages of occurances of tasks and has one column for weighted percents based multiple occurrences, and one where that weight is normalized\n",
    "        It then sorts it based on the O*NET-SOC Code.\n",
    "        This originally did not create a column for the weighted percentage of occurrences.\n",
    "    \n",
    "    Args:\n",
    "        task_soc_df (pd.DataFrame): DataFrame containing O*NET tasks and SOC titles.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with percentage of occurrences added.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load task mappings from Claude data\n",
    "    task_mappings_df = pd.read_csv(\"../original_data/onet_task_mappings.csv\")\n",
    "    \n",
    "    # Merge with existing task DataFrame\n",
    "    merged = task_mappings_df.merge(\n",
    "        df,\n",
    "        left_on=\"task_name\",\n",
    "        right_on=\"task_normalized\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Calculate weighted and normalized percentages\n",
    "    merged[\"pct_occ_weighted\"] = 100 * merged[\"pct\"] / merged[\"pct\"].sum()\n",
    "    merged[\"pct_occ_norm\"] = 100 * (merged[\"pct\"] / merged[\"n_occurrences\"]) / (merged[\"pct\"] / merged[\"n_occurrences\"]).sum()\n",
    "    \n",
    "    # Sort by O*NET-SOC Code\n",
    "    merged.sort_values(by=\"occ_group_code\", ascending=True, inplace=True)\n",
    "    \n",
    "    return merged\n",
    "\n",
    "task_soc_pct_all = add_claude_pct(task_soc_df)\n",
    "#display(task_soc_pct_all.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c465f",
   "metadata": {},
   "source": [
    "## Extra Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2940280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_emp_wage_data(df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This loads in the employment wage data  and merges it into the given dataframe with the desired columns on the occupation code.\n",
    "        If a row doesn't match, we will fall back to merging on occupation title. \n",
    "        All column names in the resulting DataFrame will be lowercase.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input the df with the ONET and Claude data merged.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Merged DataFrame with employment and wage data\n",
    "    \"\"\"\n",
    "    emp_wage_df = pd.read_csv(\"../extra_data/emp_wage_national.csv\")\n",
    "\n",
    "    # Standardize for merges\n",
    "    df[\"occ_group_code\"] = df[\"occ_group_code\"].str[:7]\n",
    "    df[\"title_normalized\"] = df[\"title\"].str.lower().str.strip()\n",
    "    emp_wage_df[\"occ_title_normalized\"] = emp_wage_df[\"OCC_TITLE\"].str.lower().str.strip()\n",
    "\n",
    "    wage_cols = [\n",
    "            \"OCC_CODE\", \"AREA_TITLE\", \"TOT_EMP\", \"EMP_PRSE\", \"JOBS_1000\",\n",
    "            \"LOC_QUOTIENT\", \"PCT_TOTAL\", \"PCT_RPT\", \"H_MEAN\", \"A_MEAN\",\n",
    "            \"MEAN_PRSE\", \"H_PCT10\", \"H_PCT25\", \"H_MEDIAN\", \"H_PCT75\", \"H_PCT90\",\n",
    "            \"A_PCT10\", \"A_PCT25\", \"A_MEDIAN\", \"A_PCT75\", \"A_PCT90\", \"ANNUAL\", \"HOURLY\", \"occ_title_normalized\"\n",
    "        ]\n",
    "\n",
    "    # Perform merge\n",
    "    merged_df = pd.merge(\n",
    "        df,\n",
    "        emp_wage_df[wage_cols],\n",
    "        left_on=\"occ_group_code\",\n",
    "        right_on=\"OCC_CODE\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    merged_matched = merged_df[merged_df[\"TOT_EMP\"].notna()]\n",
    "    unmatched = merged_df[merged_df[\"TOT_EMP\"].isna()]\n",
    "    unmatched = unmatched.drop(columns=wage_cols, errors=\"ignore\")\n",
    "\n",
    "    merged_unmatched = pd.merge(\n",
    "        unmatched,\n",
    "        emp_wage_df[wage_cols],\n",
    "        left_on=\"title_normalized\",\n",
    "        right_on=\"occ_title_normalized\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    final_merged = pd.concat([merged_matched, merged_unmatched], ignore_index=True)\n",
    "    final_merged.drop(columns=[\"title_normalized\", \"occ_title_normalized\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "\n",
    "    # Convert all column names to lowercase\n",
    "    final_merged.columns = [col.lower() for col in final_merged.columns]\n",
    "\n",
    "    return final_merged\n",
    "\n",
    "task_emp_wage_df = add_emp_wage_data(task_soc_pct_all)\n",
    "#display(task_emp_wage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3db9ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task ratings processing\n",
    "\n",
    "def add_task_ratings():\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This function reads the task ratings from an Excel file, processes it to extract frequency, importance, and relevance ratings,\n",
    "        and merges them into a single DataFrame with the desired structure.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input the df with the ONET, Claude, and emp and wage data merged.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Merged DataFrame with task ratings including frequency, importance, and relevance.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    task_ratings_df = pd.read_csv(\"../extra_data/task_ratings.csv\")\n",
    "\n",
    "\n",
    "    #Frequency mapping\n",
    "    frequency_weights = {\n",
    "        1: 1 / 260,\n",
    "        2: 2 / 260,\n",
    "        3: 12 / 260,\n",
    "        4: 52 / 260,\n",
    "        5: 1,\n",
    "        6: 3,\n",
    "        7: 8\n",
    "    }\n",
    "\n",
    "\n",
    "    # Get freq rows, drop unusable ones, generate freq aggregates\n",
    "    freq_df = task_ratings_df[task_ratings_df[\"Scale ID\"] == \"FT\"].copy()\n",
    "\n",
    "    # Drop rows without category or invalid categories\n",
    "    freq_df = freq_df[pd.to_numeric(freq_df[\"Category\"], errors='coerce').notnull()]\n",
    "    freq_df[\"Category\"] = freq_df[\"Category\"].astype(int)\n",
    "\n",
    "    # Apply weights\n",
    "    freq_df[\"freq_mean\"] = freq_df[\"Data Value\"] * freq_df[\"Category\"].map(frequency_weights) / 100\n",
    "    freq_df[\"freq_lower\"] = freq_df[\"Lower CI Bound\"] * freq_df[\"Category\"].map(frequency_weights) / 100\n",
    "    freq_df[\"freq_upper\"] = freq_df[\"Upper CI Bound\"] * freq_df[\"Category\"].map(frequency_weights) / 100\n",
    "\n",
    "    # Sum across categories to get per-task total\n",
    "    freq_agg = freq_df.groupby([\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\"]).agg({\n",
    "        \"freq_mean\": \"sum\",\n",
    "        \"freq_lower\": \"sum\",\n",
    "        \"freq_upper\": \"sum\"\n",
    "    }).reset_index()\n",
    "\n",
    "\n",
    "    # Get importance and relevance ratings\n",
    "    importance_df = task_ratings_df[task_ratings_df[\"Scale ID\"] == \"IM\"].copy()\n",
    "    importance_df = importance_df[[\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\", \n",
    "                                \"Data Value\", \"Lower CI Bound\", \"Upper CI Bound\"]]\n",
    "    importance_df = importance_df.rename(columns={\n",
    "        \"Data Value\": \"importance\",\n",
    "        \"Lower CI Bound\": \"importance_lower\",\n",
    "        \"Upper CI Bound\": \"importance_upper\"\n",
    "    })\n",
    "\n",
    "    relevance_df = task_ratings_df[task_ratings_df[\"Scale ID\"] == \"RT\"].copy()\n",
    "    relevance_df = relevance_df[[\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\", \n",
    "                                \"Data Value\", \"Lower CI Bound\", \"Upper CI Bound\"]]\n",
    "    relevance_df = relevance_df.rename(columns={\n",
    "        \"Data Value\": \"relevance\",\n",
    "        \"Lower CI Bound\": \"relevance_lower\",\n",
    "        \"Upper CI Bound\": \"relevance_upper\"\n",
    "    })\n",
    "\n",
    "\n",
    "    # Merge ratings\n",
    "    merged_ratings = freq_agg.merge(importance_df, on=[\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\"], how=\"left\")\n",
    "    merged_ratings = merged_ratings.merge(relevance_df, on=[\"O*NET-SOC Code\", \"Title\", \"Task ID\", \"Task\"], how=\"left\")\n",
    "\n",
    "\n",
    "    merged_ratings[\"task_normalized\"] = merged_ratings[\"Task\"].str.lower().str.strip()\n",
    "\n",
    "\n",
    "    return merged_ratings\n",
    "\n",
    "ratings_df = add_task_ratings()\n",
    "#display(ratings_df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "669e5157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge all and final cleanup\n",
    "\n",
    "def batch_lemmatize(texts):\n",
    "    \"\"\"\n",
    "    Efficiently lemmatize a list of strings using spaCy's nlp.pipe().\n",
    "    Skips punctuation, whitespace, and possessives.\n",
    "    \"\"\"\n",
    "    if not texts:\n",
    "        return []\n",
    "    \n",
    "    # Handle empty/null strings\n",
    "    processed_texts = [str(text).strip() if text and str(text).strip() else \" \" for text in texts]\n",
    "    \n",
    "    cleaned = []\n",
    "    try:\n",
    "        for doc in nlp.pipe(processed_texts, batch_size=1000, disable=[\"ner\", \"parser\"]):\n",
    "            lemmas = [\n",
    "                token.lemma_ for token in doc\n",
    "                if not token.is_punct and not token.is_space and token.text != \"'s\"\n",
    "            ]\n",
    "            result = \" \".join(lemmas).strip()\n",
    "            cleaned.append(result if result else \"\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch_lemmatize: {e}\")\n",
    "        raise\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def merge_all_and_cleanup(df, ratings_df):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This function merges the task data with the ratings data and performs final cleanup.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing task data.\n",
    "        ratings_df (pd.DataFrame): DataFrame containing task ratings.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Final merged DataFrame with all necessary information.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalize task names\n",
    "\n",
    "    # Apply batch lemmatization\n",
    "    df[\"task_normalized\"] = batch_lemmatize(df[\"task\"].tolist())\n",
    "    ratings_df[\"task_normalized\"] = batch_lemmatize(ratings_df[\"Task\"].tolist())\n",
    "\n",
    "    df[\"title_normalized\"] = df[\"title\"].str.lower().str.strip()\n",
    "    ratings_df[\"title_normalized\"] = ratings_df[\"Title\"].str.lower().str.strip()\n",
    "\n",
    "    # Count how many times each normalized task appears\n",
    "    task_counts = df[\"task_normalized\"].value_counts()\n",
    "\n",
    "    # Boolean mask for duplicate vs. unique tasks\n",
    "    is_duplicate = df[\"task_normalized\"].isin(task_counts[task_counts > 1].index)\n",
    "    is_unique = ~is_duplicate\n",
    "\n",
    "    # Split the dataframe\n",
    "    df_duplicate_tasks = df[is_duplicate].copy()\n",
    "    df_unique_tasks = df[is_unique].copy()\n",
    "\n",
    "    # Count how many times each normalized task appears\n",
    "    task_counts_ratings = ratings_df[\"task_normalized\"].value_counts()\n",
    "\n",
    "    # Boolean mask for duplicate vs. unique tasks\n",
    "    is_duplicate_ratings = ratings_df[\"task_normalized\"].isin(task_counts_ratings[task_counts_ratings > 1].index)\n",
    "    is_unique_ratings = ~is_duplicate_ratings\n",
    "\n",
    "    # Split the dataframe\n",
    "    df_duplicate_tasks_ratings = ratings_df[is_duplicate_ratings].copy()\n",
    "    df_unique_tasks_ratings = ratings_df[is_unique_ratings].copy()\n",
    "\n",
    "    # Merge on unique tasks\n",
    "    merged_unique = df_unique_tasks.merge(\n",
    "        df_unique_tasks_ratings[[\n",
    "            \"freq_mean\", \"freq_lower\", \"freq_upper\",\n",
    "            \"importance\", \"importance_lower\", \"importance_upper\",\n",
    "            \"relevance\", \"relevance_lower\", \"relevance_upper\",\n",
    "            \"task_normalized\", \"title_normalized\"\n",
    "        ]],\n",
    "        on=[\"task_normalized\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # Merge on both title and task\n",
    "    merged_duplicate = df_duplicate_tasks.merge(\n",
    "        df_duplicate_tasks_ratings[[\n",
    "            \"freq_mean\", \"freq_lower\", \"freq_upper\",\n",
    "            \"importance\", \"importance_lower\", \"importance_upper\",\n",
    "            \"relevance\", \"relevance_lower\", \"relevance_upper\",\n",
    "            \"task_normalized\", \"title_normalized\"\n",
    "        ]],\n",
    "        on=[\"task_normalized\", \"title_normalized\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    merged = pd.concat([merged_unique, merged_duplicate], ignore_index=True)\n",
    "\n",
    "    # Replace placeholders with NaN\n",
    "    placeholder_values = [\"#\", \"*\", \"\", \"n/a\", \"na\", \"--\"]\n",
    "    merged.replace(placeholder_values, pd.NA, inplace=True)\n",
    "\n",
    "    # Drop fully empty columns\n",
    "    merged.dropna(axis=1, how=\"all\", inplace=True)\n",
    "\n",
    "    # Drop 'occ_code' and 'task_name'\n",
    "    merged.drop(columns=[\"occ_code\", \"task_name\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # Reorder columns: make 'task' and 'task_normalized' first\n",
    "    cols = merged.columns.tolist()\n",
    "    for col in [\"task_normalized\", \"task\"]:\n",
    "        if col in cols:\n",
    "            cols.insert(0, cols.pop(cols.index(col)))\n",
    "    merged = merged[cols]\n",
    "\n",
    "    return merged\n",
    "\n",
    "task_final = merge_all_and_cleanup(task_emp_wage_df, ratings_df)\n",
    "task_final.to_csv(\"../new_data/tasks_final.csv\", index=False)\n",
    "#display(task_final.reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0779eaf0",
   "metadata": {},
   "source": [
    "## TBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7933e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It then creates separate DataFrames for core and supplemental tasks\n",
    "# It then saves the data to csv files in the\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Create DataFrame for Supplemental tasks\n",
    "# task_soc_pct_suppl_df = task_soc_pct_all[task_soc_pct_all[\"Task Type\"] == \"Supplemental\"].copy()\n",
    "\n",
    "# # Create DataFrame for Core tasks\n",
    "# task_soc_pct_core_df = task_soc_pct_all[task_soc_pct_all[\"Task Type\"] == \"Core\"].copy()\n",
    "\n",
    "# # Dictionary of dataframes and their names\n",
    "# dfs = {\n",
    "#     \"grouped_with_occupations_all\": task_soc_pct_all,\n",
    "#     \"gwo_core_df\": task_soc_pct_core_df,\n",
    "#     \"gwo_suppl_df\": task_soc_pct_suppl_df,\n",
    "# }\n",
    "\n",
    "# for name, df in dfs.items():\n",
    "#     # Normalize weighted percentages\n",
    "#     df[\"pct_occ_weighted\"] = 100 * df[\"pct\"] / df[\"pct\"].sum()\n",
    "\n",
    "#     # Normalize percentages\n",
    "#     df[\"pct_occ_norm\"] = 100 * (df[\"pct\"] / df[\"n_occurrences\"]) / (df[\"pct\"] / df[\"n_occurrences\"]).sum()\n",
    "\n",
    "#     # Print check\n",
    "#     print(f\"{name} — Raw Sum: {df['pct_occ_weighted'].sum():.2f}, Spread Sum: {df['pct_occ_norm'].sum():.2f}\")\n",
    "\n",
    "#     # Save CSV\n",
    "#     path = f\"../new_generated_data/{name}.csv\"\n",
    "#     if os.path.exists(path):\n",
    "#         try:\n",
    "#             os.remove(path)\n",
    "#         except PermissionError:\n",
    "#             print(f\"⚠️ Close {path} before saving.\")\n",
    "#     df.to_csv(path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
